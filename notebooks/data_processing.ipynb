{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:18:50.777736127Z",
     "start_time": "2024-01-04T22:18:50.732976719Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder = Path('data')\n",
    "if not (data_folder / 'Final Project data').exists():\n",
    "    # Download and extract files\n",
    "    from urllib.request import urlretrieve\n",
    "    from zipfile import ZipFile\n",
    "\n",
    "    data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    zip_path = data_folder / 'Final Project data.zip'\n",
    "    if not zip_path.exists() or zip_path.stat().st_size != 4874175804:\n",
    "        def make_progress_hook():\n",
    "            \"\"\"Returns function hook to print download progress\"\"\"\n",
    "            from time import perf_counter\n",
    "            from sys import stdout\n",
    "\n",
    "            def _progress_hook(count, block_size, total_size):\n",
    "                nonlocal last_update, local_size, ema_speed, ema_size\n",
    "\n",
    "                current_time = perf_counter()\n",
    "                local_size += block_size\n",
    "                time_interval = current_time - last_update\n",
    "                if local_size >= total_size or time_interval > update_interval:\n",
    "                    speed = (local_size - ema_size) / time_interval\n",
    "                    ema_speed = ema_alpha * speed + (1 - ema_alpha) * ema_speed if local_size > 3e7 else speed\n",
    "                    mins, secs = divmod(min(int((total_size - local_size) / ema_speed), 5999), 60)\n",
    "                    last_update, ema_size = current_time, local_size\n",
    "                    stdout.write(f\"\\rDownloading: {int(100 * local_size / total_size): 3}% [ETA: {mins:02}:{secs:02}] \")\n",
    "                    stdout.flush()\n",
    "\n",
    "            ema_alpha = 0.005\n",
    "            update_interval = 0.2\n",
    "            last_update = perf_counter()\n",
    "            local_size = ema_speed = ema_size = 0\n",
    "            return _progress_hook\n",
    "\n",
    "\n",
    "        zip_url = 'https://remmar.s3.eu-central-1.amazonaws.com/Final+Project+data.zip'\n",
    "        urlretrieve(zip_url, zip_path, reporthook=make_progress_hook())\n",
    "        print('')\n",
    "\n",
    "    print('Extracting...')\n",
    "    with ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(248, 35624)\n"
     ]
    }
   ],
   "source": [
    "from data import load_file\n",
    "data, label = load_file(file_name_path= \"data/Final Project data/Intra/train/task_motor_105923_1.h5\")\n",
    "print(label)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:18:52.724270782Z",
     "start_time": "2024-01-04T22:18:50.889943846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35848824,  1.09445492,  2.10665066, ...,  0.73223247,\n",
       "         0.68130328,  0.51004145],\n",
       "       [-1.65286332, -1.90188809, -2.03713003, ..., -1.57736323,\n",
       "        -1.54807689, -1.4020068 ],\n",
       "       [ 0.75907758,  0.79060384,  0.82546458, ...,  0.84476225,\n",
       "         0.66525369,  0.46801558],\n",
       "       ...,\n",
       "       [ 1.47625062,  1.74331856,  2.24407067, ...,  0.65807016,\n",
       "         0.66537606,  0.72636419],\n",
       "       [-1.26970342, -1.3127728 , -1.35584218, ..., -1.28935972,\n",
       "        -1.34538767, -1.40348433],\n",
       "       [ 0.96017504,  0.92555824,  0.9406922 , ...,  0.66420581,\n",
       "         0.83909806,  0.72412342]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import preprocess_meg_sample\n",
    "file_name_path = \"data/Final Project data/Cross/train/task_working_memory_164636_7.h5\"\n",
    "data, label = load_file(file_name_path)\n",
    "preprocess_meg_sample(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_data_loaders\n",
    "intra_loader_train, intra_loader_test = get_data_loaders(type=\"intra-subject\", batch_size=2)\n",
    "cross_loader_train, cross_loader_test = get_data_loaders(type=\"cross-subject\", batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.011\n",
      "[1,     6] loss: 0.054\n",
      "[1,    11] loss: 0.061\n",
      "[1,    16] loss: 0.045\n",
      "Train Acc (epoch=0): 23/32\t0.719\n",
      "Val Acc: 2/8\t0.250\n",
      "[2,     1] loss: 0.269\n",
      "[2,     6] loss: 0.015\n",
      "[2,    11] loss: nan\n",
      "[2,    16] loss: nan\n",
      "Train Acc (epoch=1): 12/32\t0.375\n",
      "Val Acc: 2/8\t0.250\n",
      "[3,     1] loss: nan\n",
      "[3,     6] loss: nan\n",
      "[3,    11] loss: nan\n",
      "[3,    16] loss: nan\n",
      "Train Acc (epoch=2): 8/32\t0.250\n",
      "Val Acc: 2/8\t0.250\n",
      "[4,     1] loss: nan\n",
      "[4,     6] loss: nan\n",
      "[4,    11] loss: nan\n",
      "[4,    16] loss: nan\n",
      "Train Acc (epoch=3): 8/32\t0.250\n",
      "Val Acc: 2/8\t0.250\n",
      "[5,     1] loss: nan\n",
      "[5,     6] loss: nan\n",
      "[5,    11] loss: nan\n",
      "[5,    16] loss: nan\n",
      "Train Acc (epoch=4): 8/32\t0.250\n",
      "Val Acc: 2/8\t0.250\n",
      "[6,     1] loss: nan\n",
      "[6,     6] loss: nan\n",
      "[6,    11] loss: nan\n",
      "[6,    16] loss: nan\n",
      "Train Acc (epoch=5): 8/32\t0.250\n",
      "Val Acc: 2/8\t0.250\n",
      "[7,     1] loss: nan\n",
      "[7,     6] loss: nan\n",
      "[7,    11] loss: nan\n",
      "[7,    16] loss: nan\n",
      "Train Acc (epoch=6): 8/32\t0.250\n",
      "Val Acc: 2/8\t0.250\n",
      "[8,     1] loss: nan\n",
      "[8,     6] loss: nan\n",
      "[8,    11] loss: nan\n",
      "[8,    16] loss: nan\n",
      "Train Acc (epoch=7): 8/32\t0.250\n",
      "Val Acc: 2/8\t0.250\n",
      "[9,     1] loss: nan\n",
      "[9,     6] loss: nan\n",
      "[9,    11] loss: nan\n",
      "[9,    16] loss: nan\n",
      "Train Acc (epoch=8): 8/32\t0.250\n",
      "Val Acc: 2/8\t0.250\n",
      "[10,     1] loss: nan\n",
      "[10,     6] loss: nan\n",
      "[10,    11] loss: nan\n",
      "[10,    16] loss: nan\n",
      "Train Acc (epoch=9): 8/32\t0.250\n",
      "Val Acc: 2/8\t0.250\n"
     ]
    }
   ],
   "source": [
    "from models import RNN\n",
    "from train import train_and_validate\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch import device, cuda\n",
    "dev = device('cuda:0' if cuda.is_available() else 'cpu')\n",
    "\n",
    "LEARNING_RATE= 0.1\n",
    "\n",
    "rnn = RNN(input_size=248, hidden_size=248, num_layers=1, output_size=4).to(dev)\n",
    "cross_entropy = CrossEntropyLoss()\n",
    "optimizer = Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = SGD(rnn.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "train_and_validate(criterion=cross_entropy, optimizer=optimizer, model=rnn, scheduler=scheduler, \n",
    "                   dataloader_train=intra_loader_train, dataloader_test=intra_loader_test, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BaselineCNN\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "sample = next(iter(intra_loader_train))[0]\n",
    "model = BaselineCNN(sample).to(dev)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
